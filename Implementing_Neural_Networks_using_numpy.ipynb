{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing Neural Networks using numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM03HCYCQAwenuig2whECQH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsi1609/ANN-with-only-numpy/blob/main/Implementing_Neural_Networks_using_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSE6GqUEjfMC"
      },
      "source": [
        "# ANN implementation using only NumPy and experimenting with activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaMXKO1Ijqff"
      },
      "source": [
        "## Basic ANN\r\n",
        "ANN's are the most fundamental structure of neural networks. The basic ANN structure is known as perceptron. Perceptron is a simple linear regression with an activation function.  Linear Regression is applied for finding the linear relationship between input and output. But most of the real data is non-linear in nature, so to make the regression versatile, we use perceptron with activation. The activation function adds non-linearity to the output making it flexible for non-linear data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgeCX5Ximh2X"
      },
      "source": [
        "## Importing Essentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJjpV3vBjj3I"
      },
      "source": [
        "from sklearn.datasets import fetch_openml #Downloading the MNIST dataset\r\n",
        "from keras.utils.np_utils import to_categorical #One hot encoding of labels \r\n",
        "import numpy as np #For linear algebra\r\n",
        "from sklearn.model_selection import train_test_split #Spliting the dataset  \r\n",
        "import time #For keeping track of time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdlqQWMYmky_"
      },
      "source": [
        "## Normalizing and spliting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQX2Clb6kijH"
      },
      "source": [
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True) # Fetching the mnist data\r\n",
        "x = (x/255).astype('float32')# Normalising the values\r\n",
        "y = to_categorical(y) # One hot encoding\r\n",
        "# Train and validation split \r\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-J1MdyLk7tX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}